{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Information about the submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Name and number of the assignment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Detoxification. Assignment 3.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Student name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nikolay Kalmykov**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Codalab user ID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nick**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Technical Report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Methodology "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use `sberbank-ai/ruT5-base` model (222M parameters), that was trained on a large corpus of Russian text using a denoising auto-encoder objective.\n",
    "\n",
    "Also, in this task, I decided to implement the algorithm of Machine Translation for English Parallel dataset (https://huggingface.co/datasets/s-nlp/paradetox). \n",
    "\n",
    "The exact steps:\n",
    "\n",
    "1) The code uses the `MarianMTModel` and `MarianTokenizer` to translate each comment in the \"toxic_comment\" and \"neutral_comment\" columns of the DataFrame from English to Russian.\n",
    "\n",
    "2) Then, the code defines a PyTorch data collator class called \"DataCollatorWithPadding\" to pad and batch the pairs of comments.\n",
    "\n",
    "3) The code uses an Adam optimizer and gradient accumulation to save GPU memory. It also computes the exponential moving average of the loss and reports the average loss every \"window\" steps.\n",
    "\n",
    "\n",
    "* Then, I used the next hyperparameters for training:\n",
    "   \n",
    "  * batch_size = 18\n",
    "  * num_epoch = 15\n",
    "  * learning_rate = 3e-5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Discussion of results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Method | Style transfer accuracy | Meaning preservation | Fluency | Joint score | ChrF1| \n",
    "--- | --- | --- |--- | --- |--- | \n",
    "Baseline | 0.56 | 0.89 | 0.85 | 0.41 | 0.53\n",
    "T5 (Machine Translation of Eng Parallel dataset) | 0.66 | 0.73 | 0.85 | 0.40 | 0.46\n",
    "T5 (Extended dataset) |0.78 | 0.82 | 0.82 | 0.53 | 0.56\n",
    "\n",
    "For dataset got with Machine translation, the results weren't so good. Probably, it happens because Machine Translation Systems are not perfect and can make errors while translating. These errors could remove important information that would help to detect toxicity. Also, Machine translation systems sometimes lose the context of the original text while translating, which could make it difficult to detect the presence of toxic language or understand the meaning of the text. Finally, the presence of certain words or phrases in the original text may not be present in the translated text.\n",
    "\n",
    "So, I used also mixed (Extended dataset) with translated sentences and from original Russian Parallel Data. The scoe was higher, but also not higher than the score for model trained only on Russian Parallel data.\n",
    "\n",
    "To improve, the score with Machine Translation algorithm, it may be review and annotate the translated dataset to ensure the quality of the data. But, this is routine work and it is better to try other models like Conditional Bert, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers -q\n",
    "# !wget https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/train.tsv -q\n",
    "# !wget https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/dev.tsv -q\n",
    "# !wget https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/test.tsv -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, MarianMTModel, MarianTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "from typing import Tuple, List, Dict, Union\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Machine Translation for English train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"s-nlp/paradetox\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset['train'])\n",
    "train_df.head()\n",
    "toxic_comment_en = train_df['en_toxic_comment'].tolist()\n",
    "neutral_comment_en = train_df['en_neutral_comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_transl = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-ru')\n",
    "model_transl = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "russian_toxic_sentences = []\n",
    "russian_n_sentences = []\n",
    "\n",
    "english_toxic_sentences = toxic_comment_en\n",
    "english_neural_sentences = neutral_comment_en\n",
    "\n",
    "\n",
    "for sentence_tox, sentence_n in tqdm(zip(english_toxic_sentences, english_neural_sentences)):\n",
    "\n",
    "\n",
    "    inputs_tox = tokenizer_transl(sentence_tox, return_tensors='pt', padding=True)\n",
    "    outputs_tox = model_transl.generate(**inputs_tox)\n",
    "    translated_sentence_tox = tokenizer_transl.decode(outputs_tox[0], skip_special_tokens=True)\n",
    "    \n",
    "    inputs_n = tokenizer_transl(sentence_n, return_tensors='pt', padding=True)\n",
    "    outputs_n = model_transl.generate(**inputs_n)\n",
    "    translated_sentence_n = tokenizer_transl.decode(outputs_n[0], skip_special_tokens=True)\n",
    "    \n",
    "    russian_toxic_sentences.append(translated_sentence_tox)\n",
    "    russian_n_sentences.append(translated_sentence_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ls_1 = russian_toxic_sentences\n",
    "ls_2 = russian_n_sentences\n",
    "\n",
    "df = pd.DataFrame(list(zip(ls_1, ls_2)), columns=['toxic_comment', 'neutral_comment'])\n",
    "df.to_csv('train_translated.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('train.tsv', sep='\\t', index_col=0)\n",
    "df = df.fillna('')\n",
    "\n",
    "\n",
    "df_train_toxic = []\n",
    "df_train_neutral = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    references = row[['neutral_comment1', 'neutral_comment2', 'neutral_comment3']].tolist()\n",
    "    \n",
    "    for reference in references:\n",
    "\n",
    "        if len(reference) > 0:\n",
    "            df_train_toxic.append(row['toxic_comment'])\n",
    "            df_train_neutral.append(reference)\n",
    "            \n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>и,чё,блядь где этот херой был до этого со свои...</td>\n",
       "      <td>Ну и где этот герой был,со своими доказательст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>и,чё,блядь где этот херой был до этого со свои...</td>\n",
       "      <td>Где этот герой был до этого со своими доказате...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>и,чё,блядь где этот херой был до этого со свои...</td>\n",
       "      <td>и,где этот герой был до этого со своими доказа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>О, а есть деанон этого петуха?</td>\n",
       "      <td>О, а есть деанон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>херну всякую пишут,из-за этого лайка.долбоебизм.</td>\n",
       "      <td>Чушь всякую пишут, из- за этого лайка.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       toxic_comment  \\\n",
       "0  и,чё,блядь где этот херой был до этого со свои...   \n",
       "1  и,чё,блядь где этот херой был до этого со свои...   \n",
       "2  и,чё,блядь где этот херой был до этого со свои...   \n",
       "3                     О, а есть деанон этого петуха?   \n",
       "4   херну всякую пишут,из-за этого лайка.долбоебизм.   \n",
       "\n",
       "                                     neutral_comment  \n",
       "0  Ну и где этот герой был,со своими доказательст...  \n",
       "1  Где этот герой был до этого со своими доказате...  \n",
       "2  и,где этот герой был до этого со своими доказа...  \n",
       "3                                   О, а есть деанон  \n",
       "4             Чушь всякую пишут, из- за этого лайка.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_1 = pd.DataFrame({\n",
    "    'toxic_comment': df_train_toxic,\n",
    "    'neutral_comment': df_train_neutral\n",
    "})\n",
    "df_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>У него тоже были стальные яйца!</td>\n",
       "      <td>Он тоже был храбрым!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Чувак должен был быть в Апи, он был бы прямо д...</td>\n",
       "      <td>Было бы неплохо, если бы он пошёл в Апи.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Я не собираюсь продавать эту чертову фотографи...</td>\n",
       "      <td>Я не собираюсь продавать фотографию, я просто ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Отбросы, которые создают КНН и другие агентств...</td>\n",
       "      <td>Новости, которые создают КНН и другие новостны...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Причина, по которой их не существует, в том, ч...</td>\n",
       "      <td>Причина, по которой их не существует, в том, ч...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       toxic_comment  \\\n",
       "0                    У него тоже были стальные яйца!   \n",
       "1  Чувак должен был быть в Апи, он был бы прямо д...   \n",
       "2  Я не собираюсь продавать эту чертову фотографи...   \n",
       "3  Отбросы, которые создают КНН и другие агентств...   \n",
       "4  Причина, по которой их не существует, в том, ч...   \n",
       "\n",
       "                                     neutral_comment  \n",
       "0                               Он тоже был храбрым!  \n",
       "1           Было бы неплохо, если бы он пошёл в Апи.  \n",
       "2  Я не собираюсь продавать фотографию, я просто ...  \n",
       "3  Новости, которые создают КНН и другие новостны...  \n",
       "4  Причина, по которой их не существует, в том, ч...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_2 = pd.read_csv('train_translated.tsv', sep=\"\\t\")\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30834, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15752</th>\n",
       "      <td>Только что переехал в Ванкувер Ва, оставив все...</td>\n",
       "      <td>Просто переехал в Ванкувер Ва, оставив весь эт...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>завтрака хэппи милом , пиздец поправилась ;(</td>\n",
       "      <td>завтрака хэппи милом и поправилась</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Она такая лживая сволочь.</td>\n",
       "      <td>Она лжет.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Лол-на-я обнаружила, что девчонки из моего дом...</td>\n",
       "      <td>Ну, нет, я обнаружила, что когда мои домашние ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>бля, сука.... почему нет такого человека, кото...</td>\n",
       "      <td>почему нет такого человека, которому бы я смог...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           toxic_comment  \\\n",
       "15752  Только что переехал в Ванкувер Ва, оставив все...   \n",
       "5076        завтрака хэппи милом , пиздец поправилась ;(   \n",
       "730                            Она такая лживая сволочь.   \n",
       "4997   Лол-на-я обнаружила, что девчонки из моего дом...   \n",
       "8716   бля, сука.... почему нет такого человека, кото...   \n",
       "\n",
       "                                         neutral_comment  \n",
       "15752  Просто переехал в Ванкувер Ва, оставив весь эт...  \n",
       "5076                  завтрака хэппи милом и поправилась  \n",
       "730                                            Она лжет.  \n",
       "4997   Ну, нет, я обнаружила, что когда мои домашние ...  \n",
       "8716   почему нет такого человека, которому бы я смог...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.concat([df_1, df_2], axis=0)\n",
    "df = shuffle(df)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PairsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self.x['input_ids'])\n",
    "        item = {key: val[idx] for key, val in self.x.items()}\n",
    "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
    "        item['labels'] = self.y['input_ids'][idx]\n",
    "        return item\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self.x['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n # * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "        )\n",
    "        ybatch = self.tokenizer.pad(\n",
    "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
    "            padding=True,\n",
    "        ) \n",
    "        batch['labels'] = ybatch['input_ids']\n",
    "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
    "        \n",
    "        return {k: torch.tensor(v) for k, v in batch.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    num = 0\n",
    "    den = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "            num += len(batch) * loss.item()\n",
    "            den += len(batch)\n",
    "    val_loss = num / den\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model, train_dataloader, val_dataloader, \n",
    "    max_epochs=30, \n",
    "    max_steps=1_000, \n",
    "    lr=3e-5,\n",
    "    gradient_accumulation_steps=1, \n",
    "    cleanup_step=100,\n",
    "    report_step=300,\n",
    "    window=100,\n",
    "):\n",
    "    cleanup()\n",
    "    optimizer = torch.optim.Adam(params = [p for p in model.parameters() if p.requires_grad], lr=lr)\n",
    "\n",
    "    ewm_loss = 0\n",
    "    step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in trange(max_epochs):\n",
    "        print(step, max_steps)\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "        tq = tqdm(train_dataloader)\n",
    "        for i, batch in enumerate(tq):\n",
    "            try:\n",
    "                batch['labels'][batch['labels']==0] = -100\n",
    "                loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print('error on step', i, e)\n",
    "                loss = None\n",
    "                cleanup()\n",
    "                continue\n",
    "            if i and i % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                step += 1\n",
    "                if step >= max_steps:\n",
    "                    break\n",
    "\n",
    "            if i % cleanup_step == 0:\n",
    "                cleanup()\n",
    "\n",
    "            w = 1 / min(i+1, window)\n",
    "            ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "            tq.set_description(f'loss: {ewm_loss:4.4f}')\n",
    "\n",
    "            if (i and i % report_step == 0 or i == len(train_dataloader)-1)  and val_dataloader is not None:\n",
    "                model.eval()\n",
    "                eval_loss = evaluate_model(model, val_dataloader)\n",
    "                model.train()\n",
    "                print(f'epoch {epoch}, step {i}/{step}: train loss: {ewm_loss:4.4f}  val loss: {eval_loss:4.4f}')\n",
    "                \n",
    "            if step % 1000 == 0:\n",
    "                model.save_pretrained(f't5_base_train_10000')\n",
    "        \n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(x, y, model_name, test_size=0.1, batch_size=32, **kwargs):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    x1, x2, y1, y2 = train_test_split(x, y, test_size=test_size, random_state=42)\n",
    "    train_dataset = PairsDataset(tokenizer(x1), tokenizer(y1))\n",
    "    test_dataset = PairsDataset(tokenizer(x2), tokenizer(y2))\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "    val_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "    train_loop(model, train_dataloader, val_dataloader, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'sberbank-ai/ruT5-base'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "steps = 10000\n",
    "print(f'\\n\\n\\n  train  {steps} \\n=====================\\n\\n')\n",
    "model = train_model(df['toxic_comment'].tolist(), df['neutral_comment'].tolist(), model_name=model_name, batch_size=18, max_epochs=15, max_steps=steps)\n",
    "model.save_pretrained(f't5_base_train_10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.tsv', sep='\\t')\n",
    "toxic_inputs = df['toxic_comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "state_dict = torch.load('/home/nikolay_kalm/EDA_OCR/NLP/Task_3/t5_base_train_10000/pytorch_model.bin')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, model, n=None, max_length='auto', temperature=0.0, beams=3):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(model.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = int(inputs.shape[1] * 1.2) + 10\n",
    "    result = model.generate(\n",
    "        inputs, \n",
    "        num_return_sequences=n or 1, \n",
    "        do_sample=False, \n",
    "        temperature=temperature, \n",
    "        repetition_penalty=3.0, \n",
    "        max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Уходи.']\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase(['Пошел нахуй'], model, temperature=50.0, beams=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Я сейчас пью кофе и мне так хорошо, по всему телу энергия проходит, и такое чувство что даже простой разговор с тобой становится в 10 раз лучше']\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase(['Я сейчас пью кофе и мне так ахуено, по всему телу энергия проходит, и такое чувство что даже простой разговор с тобой становится в 10 раз лучше'], model, temperature=50.0, beams=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Я устал жить в зиме']\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase(['Блять я заебался жить в зиме'], model, temperature=50.0, beams=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_results = []\n",
    "batch_size = 8\n",
    "\n",
    "for i in tqdm(range(0, len(toxic_inputs), batch_size)):\n",
    "    batch = [sentence for sentence in toxic_inputs[i:i + batch_size]]\n",
    "    try:\n",
    "        para_results.extend(paraphrase(batch, model, temperature=0.0))\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        para_results.append(toxic_inputs[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t5_test.txt', 'w') as file:\n",
    "    file.writelines([sentence+'\\n' for sentence in para_results])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
